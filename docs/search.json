[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "ESS 330: Lab 6",
    "section": "",
    "text": "#Attaching packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.1     \n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n\n#Downloading PDF and data\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n#Question 1\n\n#zero_q_freq represents the frequency of days with Q=0 mm/day, reported as a percentage\n\n##Exploratory data analysis\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n#Question 2\n\n#Map 1 for aridity \nmap_aridity &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"cornsilk\", high = \"red4\") +\n  ggthemes::theme_map() + labs(title = \"Aridity\")\n\n#Map 2 for p_mean\nmap_p_mean &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue2\", high = \"darkblue\") +\n  ggthemes::theme_map() + labs(title = \"Mean Daily Precipitation\")\n\nlibrary(patchwork)\nmap_aridity + map_p_mean\n\n\n\n\n\n\n\n\n#Model Preparation\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n#Visual EDA\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#Model Building\n\nset.seed(123)\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n#Model Evaluation\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n#Using a workflow\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n#Making Predictions + Model Evaluation\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n#Switch it Up\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n#Predictions and Model Evaluation\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.588\n2 rsq     standard       0.740\n3 mae     standard       0.365\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n#Workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.564  0.0253    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0260    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n#Build a xgboost (engine) regression (mode) model using boost_tree\nset.seed(123) \ndata_split &lt;- initial_split(camels, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nxgb_model &lt;- boost_tree(\n  mode = \"regression\",  \n  trees = 1000,         \n  tree_depth = 6,         \n  learn_rate = 0.1,       \n  mtry = 5,               \n  min_n = 10) %&gt;%\n  set_engine(\"xgboost\")   \n\n#Build neural network model using the nnet engine from the baguette package using the bag_mlp function\nnn_model &lt;- bag_mlp(\n  mode = \"regression\",   \n  hidden_units = 10,     \n  epochs = 100,          \n  penalty = 0.01) %&gt;%\n  set_engine(\"nnet\")  \n\n#Add workflows for each model\nworkflow_xgb &lt;- workflow() %&gt;%\n  add_model(xgb_model) %&gt;%\n  add_recipe(rec)\n\nworkflow_nn &lt;- workflow() %&gt;%\n  add_model(nn_model) %&gt;%\n  add_recipe(rec)\n\n#Train models\nxgb_fit &lt;- fit(workflow_xgb, data = train_data)\nnn_fit &lt;- fit(workflow_nn, data = train_data)\nlm_fit &lt;- fit(lm_wf, data = train_data)\nrf_fit &lt;- fit(rf_wf, data = train_data)\n\n#Predictions for models\nxgb_preds &lt;- predict(xgb_fit, new_data = test_data)\nnn_preds &lt;- predict(nn_fit, new_data = test_data)\nlm_preds &lt;- predict(lm_fit, new_data = test_data)\nrf_preds &lt;- predict(rf_fit, new_data = test_data)\n\n#Combine predictions and true values\nxgb_results &lt;- bind_cols(test_data, xgb_preds)\nnn_results &lt;- bind_cols(test_data, nn_preds)\nlm_results &lt;- bind_cols(test_data, lm_preds)\nrf_results &lt;- bind_cols(test_data, rf_preds)\n\n\n#Evaluate using R-squared\nxgb_rsq &lt;- rsq(xgb_results, truth = q_mean, estimate = .pred)\nnn_rsq &lt;- rsq(nn_results, truth = q_mean, estimate = .pred)\nlm_rsq &lt;- rsq(lm_results, truth = q_mean, estimate = .pred)\nrf_rsq &lt;- rsq(rf_results, truth = q_mean, estimate = .pred)\n\n#Print results to compare\nprint(paste(\"XGBoost Model Performance:\"))\n\n[1] \"XGBoost Model Performance:\"\n\nprint(paste(\"  R-squared: \", round(xgb_rsq$.estimate, 3)))\n\n[1] \"  R-squared:  0.453\"\n\nprint(paste(\"\\nNeural Network Model Performance:\"))\n\n[1] \"\\nNeural Network Model Performance:\"\n\nprint(paste(\"  R-squared: \", round(nn_rsq$.estimate, 3)))\n\n[1] \"  R-squared:  0.551\"\n\nprint(paste(\"\\nLinear Regression Model Performance:\"))\n\n[1] \"\\nLinear Regression Model Performance:\"\n\nprint(paste(\"  R-squared: \", round(lm_rsq$.estimate, 3)))\n\n[1] \"  R-squared:  0.599\"\n\nprint(paste(\"\\nRandom Forest Model Performance:\"))\n\n[1] \"\\nRandom Forest Model Performance:\"\n\nprint(paste(\"  R-squared: \", round(rf_rsq$.estimate, 3)))\n\n[1] \"  R-squared:  0.532\"\n\n#ANSWER: Based on the results of each model and the corresponding R-squared values, neural network is the best model to move forward with. This is because neural network has the 2nd highest R-squared value at 0.551, following linear regression. While linear regression has the highest value at 0.599, as shown previously, the data has a non-linear relationship meaning that neural network is a better model overall.\n\n#Build Your Own ##Data spliting\n\nlibrary(workflows)\nlibrary(tune)\nlibrary(rsample)\nlibrary(yardstick)\n\n#remove NA values\nlibrary(dplyr)\ncamels_train_clean &lt;- camels_train %&gt;%\n  filter(!is.na(logQmean))\nsum(is.na(camels_train_clean$logQmean))\n\n[1] 0\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels_train_clean, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n#Cross validation\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n##Recipe\n\n#Formula\nlogQmean ~ p_mean + elev_mean + slope_mean + aridity + runoff_ratio\n\nlogQmean ~ p_mean + elev_mean + slope_mean + aridity + runoff_ratio\n\n#Why this formula?\n#I am choosing this formula because each of these factors can have a significant impact on stream flow. For example, rainfall is the primary input of new stream flow within a watershed and aridity mean that stream flow is less likely. Both elevation and slope can contribute to how much runoff there is in an area, ultimately impacting stream flow. \n\n#Recipe\nrec &lt;- recipe(logQmean ~ p_mean + elev_mean + slope_mean + aridity + runoff_ratio, \n              data = camels_train) %&gt;%\nstep_normalize(all_numeric(), -all_outcomes()) %&gt;%\nstep_impute_median(all_numeric(), -all_outcomes()) \n\n##Define 3 Models\n\nrf_model &lt;- rand_forest(mode = \"regression\", trees = 500, mtry = 3, min_n = 10) %&gt;%\n  set_engine(\"ranger\")\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nxgb_model &lt;- boost_tree(mode = \"regression\", trees = 1000, tree_depth = 6, learn_rate = 0.1, min_n = 10) %&gt;%\n  set_engine(\"xgboost\")\n\n##Create Workflows\n\n#Define the workflows for each model\nworkflow_rf &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(rec)   \n\nworkflow_lm &lt;- workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(rec)  \n\nworkflow_xgb &lt;- workflow() %&gt;%\n  add_model(xgb_model) %&gt;%\n  add_recipe(rec)   \n\nmodels &lt;- list(\n  \"Random Forest\" = rf_model, \n  \"Linear Regression\" = lm_model, \n  \"XGBoost\" = xgb_model)\n\nrecipes &lt;- list(\n  \"Standard\" = rec)\n\nworkflow_set &lt;- workflow_set(preproc = recipes, \n  models = models)\n\nset.seed(123)  \n\nresults &lt;- workflow_set %&gt;%\n  workflow_map(resamples = camels_cv,\n    seed = 123,             \n    metrics = metric_set(rmse, rsq))\n\n#Evaluation\n\nautoplot(results, metric = \"rmse\") \n\n\n\n\n\n\n\nautoplot(results, metric = \"rsq\")\n\n\n\n\n\n\n\nranked_results &lt;- rank_results(results)\nprint(ranked_results)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 Standard_Random … Prepro… rmse    0.127 0.0204     10 recipe       rand…     1\n2 Standard_Random … Prepro… rsq     0.989 0.00356    10 recipe       rand…     1\n3 Standard_XGBoost  Prepro… rmse    0.162 0.0251     10 recipe       boos…     2\n4 Standard_XGBoost  Prepro… rsq     0.984 0.00405    10 recipe       boos…     2\n5 Standard_Linear … Prepro… rmse    0.420 0.0260     10 recipe       line…     3\n6 Standard_Linear … Prepro… rsq     0.883 0.00765    10 recipe       line…     3\n\n#ANSWER: Based on the results, the rand_forest model is the best because it has the highest r-squared value at 0.988\n\n##Extract and Evaluate\n\nlibrary(ggplot2)\n\n#Build model\nrf_model &lt;- rand_forest(\n  mode = \"regression\",  \n  trees = 500,          \n  mtry = 3,             \n  min_n = 10) %&gt;%\n  set_engine(\"ranger\")\n\n#Define recipe\nrec &lt;- recipe(logQmean ~ p_mean + elev_mean + slope_mean + aridity + runoff_ratio, \n              data = camels_train) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes()) %&gt;%\n  step_impute_median(all_numeric(), -all_outcomes())\n\n#Build Workflow\nworkflow_rf &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(rec)\n\n#Fit model to training data\nrf_fit &lt;- fit(workflow_rf, data = camels_train)\n\n#Make prediction on test data using augment\nrf_predictions &lt;- augment(rf_fit, new_data = camels_test)\n\n#Plot observed vs predicted\nggplot(rf_predictions, aes(x = logQmean, y = .pred)) +\n  geom_point(color = \"blue\", alpha = 0.6) +  # Blue points for observed vs predicted\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +  # Identity line\n  labs(\n    title = \"Observed vs Predicted Streamflow (logQmean)\",\n    x = \"Observed Streamflow (logQmean)\",\n    y = \"Predicted Streamflow (logQmean)\"\n  ) +\n  theme_minimal() +\n  scale_color_viridis_c()\n\n\n\n\n\n\n\n#Description of results\n#Based on the scatter plot, the data is remaining fairly close to the dashed line, showing that the model's predictions closely align with the actual observed values. While the points are pretty well distributed, there are the most points from 0.8 to 1. The strong indicator of 0.988 suggests that this is a strong model and that the Random Forest model is performing well. The model explains a significant amount of the variability in the observed stream flow."
  }
]